import requests
import csv

def fetch_data_from_api(api_endpoint, bearer_token):
    headers = {
        "Authorization": f"Bearer {bearer_token}"
    }
    response = requests.get(api_endpoint, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f"Failed to fetch data from API")
        if response.status_code == 401:
            print(f"Apply proper Bearer token!, status code : {response.status_code}")
        else:
            print(f" status code : {response.status_code}")
        return None

def format_data_type(data_type):
    if data_type == "INTEGER" or data_type == "AUTO":
        return "numeric"
    elif data_type == "VARCHAR" or data_type == "TEXT":
        return "char"
    elif data_type == "TIMESTAMP" or data_type == "TIME_STAMP":
        return "datetime"
    elif "string_var" in data_type.lower():
        return "char"
    elif data_type == "auto":
        return "numeric"
    else:
        return data_type.lower()

def write_to_csv(data, filename):
    unique_tables = []
    related_table_data = []
    
    with open(filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['SPRICED_TABLE_NAME', 'Table_Display_Name', 'TABLE_SCHEMA', 'COLUMN_NAME', 'DISPLAY_NAME', 'DATA_TYPE', 'SIZE', 'POS', 'IS_NULLABLE', 'IS_REF_COLUMN', 'PRIMARY_KEY', 'AUTOGEN', 'Ref'])
        for entity in data:
            table_name = entity["name"]
            table_display_name = entity["displayName"] 
            entity_id = entity["id"]
            attributes = entity["attributes"]
            unique_tables.append((table_name, entity_id))
            for attribute in attributes:
                column_name = attribute["name"]
                if column_name.lower() in ['id', 'updated_date', 'comment', 'updated_by', 'is_valid', 'created_date', 'created_by', 'validationstatus']:
                    continue
                display_name = attribute["displayName"]
                data_type = format_data_type(attribute["dataType"])
                size = attribute.get("size", None)
                pos = attribute.get("numberOfDecimalValues", None)
                # Ensure pos is same as size for decimal data type
                if data_type == "decimal":
                    pos = attribute.get("size", None)
                    size = 63
                is_nullable = "no" if not attribute["nullable"] else "yes"
                is_ref_column = "TRUE" if attribute["type"] == "LOOKUP" else "FALSE"
                primary_key = ""
                if attribute["constraintType"] == "UNIQUE_KEY":
                    primary_key = "UK"
                elif attribute["constraintType"] == "FOREIGN_KEY":
                    primary_key = "FK"
                elif attribute["constraintType"] == "PRIMARY_KEY":
                    primary_key = "PK"
                autogen = "true" if attribute["autoGenerated"] else ""
                ref = attribute.get("referencedTable", "")
                
                writer.writerow([table_name, table_display_name, 'spriced', column_name, display_name, data_type, size, pos, is_nullable, is_ref_column, primary_key, autogen, ref])
                
                if ref:
                    related_table_data.append([table_name, column_name, display_name, ref])
    
    print(f"All table details written to -> {filename} ,successfully!")

    # Write related_table.csv
    with open('related_tables.csv', mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['entity_name', 'columns', 'display_name', 'linked_to'])
        for row in related_table_data:
            writer.writerow(row)
    
    print(f"All related table details written to -> related_tables.csv ,successfully!")

    # Sort the unique tables by entity_id
    sorted_unique_tables = sorted(unique_tables, key=lambda x: x[1])
    
    # Write sequence_tables.csv
    with open('sequence_tables.csv', mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(['json_name', 'Sequence of Ingestion'])
        for table_name, entity_id in sorted_unique_tables:
            writer.writerow([table_name, entity_id])
    
    print(f"All unique table names written to -> sequence_tables.csv ,successfully!")

def main():
    api_endpoint = "https://spriced.meritor.uat.simadvisory.com/api/v1/definition/models/1/entities"
    bearer_token = "eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJacjNwNm9Od25oQ2xkLV9qSDJ0dWZGanBWcUhoV1ZWQlVjS0xPOG5aMVFNIn0.eyJleHAiOjE3MTg4MTMxODgsImlhdCI6MTcxODc4NDM4OSwiYXV0aF90aW1lIjoxNzE4Nzg0MzYyLCJqdGkiOiIzZmM1YmNhNC0wN2Y3LTRkYjItYjYwNC0yODM0Yjc1ZmRlNTEiLCJpc3MiOiJodHRwczovL2F1dGguc2ltYWR2aXNvcnkuY29tL2F1dGgvcmVhbG1zL1NQUklDRURfTlJQIiwiYXVkIjoiYWNjb3VudCIsInN1YiI6IjkxZjA4Nzc1LTZiMGUtNGEzMC1iYWRlLWRmN2NhOTRjMTVjMiIsInR5cCI6IkJlYXJlciIsImF6cCI6IlNQUklDRURfTlJQX0NMSUVOVCIsIm5vbmNlIjoiN2M5MzZlMmYtY2Y0OS00YjU2LTgwNTItYWJkNjcwZDEyODgxIiwic2Vzc2lvbl9zdGF0ZSI6ImZlM2JhOTVkLWYyMzEtNDc5Yi1hZDZhLWU2YWQ0NWUzMmE0NCIsImFjciI6IjAiLCJhbGxvd2VkLW9yaWdpbnMiOlsiaHR0cHM6Ly9ucnAuc3RnLnNpbWFkdmlzb3J5LmNvbSIsImh0dHBzOi8vbG9jYWxob3N0OjQyMDAiLCJodHRwczovL2xvY2FsaG9zdDozMDAwIiwiaHR0cHM6Ly9zcHJpY2VkLm1lcml0b3IucHJlLXVhdC5zaW1hZHZpc29yeS5jb20iLCJodHRwczovL25ycC51YXQuc2ltYWR2aXNvcnkuY29tIiwiKiIsImh0dHBzOi8vc3ByaWNlZC5tZXJpdG9yLnVhdC5zaW1hZHZpc29yeS5jb20iLCJodHRwczovL2Flcm8ucGF0LnNpbWFkdmlzb3J5LmNvbSIsImh0dHA6Ly9sb2NhbGhvc3Q6NDIwMCIsImh0dHBzOi8vZXhjZWxwbHVnaW4uc3ByaWNlZC5tZXJpdG9yLnVhdC5zaW1hZHZpc29yeS5jb20iLCJodHRwczovL3NwcmljZWQubWVyaXRvci5zcm8tdGVzdGluZy5zaW1hZHZpc29yeS5jb20iLCJodHRwczovL2V4Y2VscGx1Z2luLnJvdXRlLm1lcml0b3IudWF0LnNpbWFkdmlzb3J5LmNvbSJdLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiZ19NZXJpdG9yX0NvcmVfTWFuYWdlciIsIkFkbWluIiwiZGVmYXVsdC1yb2xlcy1zcHJpY2VkX25ycCJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJzaWQiOiJmZTNiYTk1ZC1mMjMxLTQ3OWItYWQ2YS1lNmFkNDVlMzJhNDQiLCJlbWFpbF92ZXJpZmllZCI6ZmFsc2UsIm5hbWUiOiJUZXN0IFVzZXIiLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJ0dTk5MSIsImdpdmVuX25hbWUiOiJUZXN0IiwiZmFtaWx5X25hbWUiOiJVc2VyIiwidGVuYW50IjoibWVyaXRvciIsImVtYWlsIjoidHU5OTFAc2ltYWR2aXNvcnkuY29tIn0.fhqxvcOi9Smxn9CAxvu6Pcl4wJCIWoMJiDFcOdrTMDqO1I6vjWjr8LtiGGHW8h6KbbC7zxXD00sOHXPvU4zdMMP5PrM9MwNw073nP2ui49OPTQ91K7paPHQ_YK8_sukl8xmBnfUvm4up3k7pxGHZKDesRgQFgyTPQFxNwJMWlfRlK4OaNx09uk8bT5rzlPuujJkIHSz5nP38gUkZ8Ggloy7s1n7UB9xRsJPNOUfGMQnXlhv5ZqXNfvY0iHuP4zx1bYcgyfgk_KWIobjy6-lMAVauqkAbteKQDHyXKk3s5JxHmzYAfWXO_bi9sCkY3olnT6UCyC3H16lG14j3LSDnAQ"  # Replace with your actual bearer token
    data = fetch_data_from_api(api_endpoint, bearer_token)
    if data:
        write_to_csv(data, "sourceDataGen.csv")
    else:
        print("No data fetched from the API.")

if __name__ == "__main__":
    main()
